{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgRPTJ0q9LqK"
      },
      "source": [
        "Next, we clone the public Github code that will help us download the data and do some preprocessing. We move the required files outside of the cloned folder to use them later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6Ge1Vqzoj-xg"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "files_to_move = ['util_functions.py', 'data_utils.py', 'preprocessing.py']\n",
        "for f in files_to_move:\n",
        "  if not os.path.exists(f):\n",
        "    shutil.move(os.path.join('IGMC', f), f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7txdJHMBG42d"
      },
      "source": [
        "Next, load the required torch and torch_geometric libraries. In addition, we load a few useful functions from the GitHub code that we've cloned above.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YnwLa71nSiY1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.nn import RGCNConv, GCNConv\n",
        "from torch_geometric.utils import dropout_adj\n",
        "from util_functions import *\n",
        "from data_utils import *\n",
        "from preprocessing import *\n",
        "\n",
        "from sklearn.metrics import precision_score, ndcg_score\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "warnings.simplefilter(\"ignore\", UserWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9kJjP0YLfvz"
      },
      "source": [
        "Define the variables: learning rate, epochs, and batch size.\n",
        "LR_DECAY_STEP and LR_DECAY_VALUE help decrease the learning rate over time to improve the training process/\n",
        "In the original experiment, I've trained the model for 80 epochs, here replacing it by 5 for the code to run fast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-bYTIuSwUobd"
      },
      "outputs": [],
      "source": [
        "# Arguments\n",
        "EPOCHS=10\n",
        "BATCH_SIZE=50\n",
        "LR=1e-3\n",
        "LR_DECAY_STEP = 20\n",
        "LR_DECAY_VALUE = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sui3tE2jL0C2"
      },
      "source": [
        "Define a seed, it will help with the reporoducibility of the results. In addition, define a device (cpu vs. cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdyLvfBPU_Vs",
        "outputId": "81c29b0a-42a3-4515-ca62-4860afdfdd82"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "device = torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(123)\n",
        "    torch.cuda.synchronize()\n",
        "    device = torch.device('cuda')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSMENitdMIAa"
      },
      "source": [
        "Use the code from the GitHub to download and clean the MovieLens 100k dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln9Rsz2mUtEw",
        "outputId": "627d6124-ed66-4ea3-879b-961aaacac6e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading ml_100k dataset\n",
            "User features shape: (943, 23)\n",
            "Item features shape: (1682, 18)\n"
          ]
        }
      ],
      "source": [
        "(u_features, v_features, adj_train, train_labels, train_u_indices, train_v_indices, val_labels,\n",
        "val_u_indices, val_v_indices, test_labels, test_u_indices, test_v_indices, class_values\n",
        ") = load_official_trainvaltest_split('ml_100k', testing=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function print(*args, sep=' ', end='\\n', file=None, flush=False)>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M23jVjpfMbuv"
      },
      "source": [
        "Next, we use the predefined code from the Github to extract an enclosing subgraph for a given graph G. This step was described in details in the section 2 of the Medium Blogpost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXPLokDtSpZG",
        "outputId": "ebe77596-3a26-4d94-cafd-b86d5e76eaba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(array([ 63, 492,  86, ..., 552, 654, 143]), array([ 435,  263,  577, ...,  495, 1559,  212]))\n"
          ]
        }
      ],
      "source": [
        "train_dataset = eval('MyDynamicDataset')(root='data/ml_100k/testmode/train', A=adj_train,\n",
        "    links=(train_u_indices, train_v_indices), labels=train_labels, h=1, sample_ratio=1.0,\n",
        "    max_nodes_per_hop=200, u_features=None, v_features=None, class_values=class_values)\n",
        "\n",
        "\n",
        "print(train_dataset.links)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(80000,)\n"
          ]
        }
      ],
      "source": [
        "print(train_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enclosing subgraph extraction begins...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing...\n",
            "100%|██████████| 40/40 [00:07<00:00,  5.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time elapsed for subgraph extraction: 7.31951904296875s\n",
            "Transforming to pytorch_geometric graphs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20000/20000 [00:02<00:00, 8079.68it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time elapsed for transforming to pytorch_geometric graphs: 2.4783411026000977s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 88%|████████▊ | 17604/20000 [00:21<00:00, 11661.92it/s]Done!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "20000"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dataset = eval('MyDataset')(root='data/ml_100k/testmode/test', A=adj_train,\n",
        "    links=(test_u_indices, test_v_indices), labels=test_labels, h=1, sample_ratio=1.0,\n",
        "    max_nodes_per_hop=200, u_features=None, v_features=None, class_values=class_values)\n",
        "\n",
        "len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(20000,)\n"
          ]
        }
      ],
      "source": [
        "print(test_u_indices.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(array([  0,   0,   0, ..., 458, 459, 461]), array([  5,   9,  11, ..., 933,   9, 681]))\n"
          ]
        }
      ],
      "source": [
        "print(test_dataset.links)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wmm2ZA8LNt7m"
      },
      "source": [
        "Now, we define the IGMC model architecture. It consists of several steps:\n",
        "\n",
        "1.  Optionally add the graph-level dropout layer. It randomly drops edges from the graph, helping avoid overfitting and making the model more robust.\n",
        "2. The message passing layer that extracts node information for each node in the subgraph. As proposed in the table, we implement it using R-GCN layer to handle different edge types.\n",
        "3. Pass it through the tanh non-linearity\n",
        "4. We stack the outputs of step 2 and 3 at each message passing layer\n",
        "5. Concatenate the node representations at each layer in the final node representation h.\n",
        "6. Pull the graph level features g by concatenating target user and item representations.\n",
        "7. Add a linear layer, ReLU non-linearity, Dropout to avoid overfitting, and final linear layer\n",
        "\n",
        "All the model parameters were chosen following the IGMC paper.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "F6QN6-_xUzlJ"
      },
      "outputs": [],
      "source": [
        "class IGMC(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IGMC, self).__init__()\n",
        "        self.rel_graph_convs = torch.nn.ModuleList()\n",
        "        self.rel_graph_convs.append(RGCNConv(in_channels=4, out_channels=32, num_relations=5, num_bases=4))\n",
        "        self.rel_graph_convs.append(RGCNConv(in_channels=32, out_channels=32, num_relations=5, num_bases=4))\n",
        "        self.rel_graph_convs.append(RGCNConv(in_channels=32, out_channels=32, num_relations=5, num_bases=4))\n",
        "        self.rel_graph_convs.append(RGCNConv(in_channels=32, out_channels=32, num_relations=5, num_bases=4))\n",
        "        self.linear_layer1 = Linear(256, 128)\n",
        "        self.linear_layer2 = Linear(128, 1)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.linear_layer1.reset_parameters()\n",
        "        self.linear_layer2.reset_parameters()\n",
        "        for i in self.rel_graph_convs:\n",
        "            i.reset_parameters()\n",
        "\n",
        "    def forward(self, data):\n",
        "        num_nodes = len(data.x)\n",
        "        edge_index_dr, edge_type_dr = dropout_adj(data.edge_index, data.edge_type, p=0.2, num_nodes=num_nodes, training=self.training)\n",
        "\n",
        "        out = data.x\n",
        "        h = []\n",
        "        for conv in self.rel_graph_convs:\n",
        "            out = conv(out, edge_index_dr, edge_type_dr)\n",
        "            out = torch.tanh(out)\n",
        "            h.append(out)\n",
        "        h = torch.cat(h, 1)\n",
        "        h = [h[data.x[:, 0] == True], h[data.x[:, 1] == True]]\n",
        "        g = torch.cat(h, 1)\n",
        "        out = self.linear_layer1(g)\n",
        "        out = F.relu(out)\n",
        "        out = F.dropout(out, p=0.5, training=self.training)\n",
        "        out = self.linear_layer2(out)\n",
        "        out = out[:,0]\n",
        "        return out\n",
        "\n",
        "model = IGMC()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjVO8ZTiR1pM"
      },
      "source": [
        "Use a DataLoader to prepare train and test data batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Zfc33AIU3Yr",
        "outputId": "7fbf2963-fcdb-490f-d551-c0c7507b8aa2"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgIzk0jhSIn-"
      },
      "source": [
        "Make sure model is using GPU. Reset the model parameters and define the optimizer. We are using Adam optimizer here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "G2xlebsTU6AS"
      },
      "outputs": [],
      "source": [
        "model.to(device)\n",
        "model.reset_parameters()\n",
        "optimizer = Adam(model.parameters(), lr=LR, weight_decay=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HIJ1i3dSR7F"
      },
      "source": [
        "Train the model for number of epochs defined at the beginning.\n",
        "At each epoch we predict the labels for the batch, find the training MSE loss, do the backpropagation step and update the learnable parameters. Print the training loss at each epoch.\n",
        "\n",
        "After each LR_DECAY_STEP we decrease the learning rate by a factor of LR_DECAY_VALUE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fxVBybWU7cU",
        "outputId": "7a3bf1bb-a80c-4170-e767-3c3cd431cf1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 ; train loss 1.2406210604310035\n",
            "epoch 2 ; train loss 1.0620688022300602\n",
            "epoch 3 ; train loss 1.0222321453318\n",
            "epoch 4 ; train loss 0.979115405548364\n",
            "epoch 5 ; train loss 0.949212177824229\n",
            "epoch 6 ; train loss 0.9278120959922671\n",
            "epoch 7 ; train loss 0.9117765583470464\n",
            "epoch 8 ; train loss 0.8991473077610135\n",
            "epoch 9 ; train loss 0.8858545563556254\n",
            "epoch 10 ; train loss 0.8802188859693706\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    train_loss_all = 0\n",
        "    for train_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        train_batch = train_batch.to(device)\n",
        "        y_pred = model(train_batch)\n",
        "        y_true = train_batch.y\n",
        "        train_loss = F.mse_loss(y_pred, y_true)\n",
        "        train_loss.backward()\n",
        "        train_loss_all += BATCH_SIZE * float(train_loss)\n",
        "        optimizer.step()\n",
        "        torch.cuda.empty_cache()\n",
        "    train_loss_all = train_loss_all / len(train_loader.dataset)\n",
        "\n",
        "    print('epoch', epoch,'; train loss', train_loss_all)\n",
        "\n",
        "    if epoch % LR_DECAY_STEP == 0:\n",
        "      for param_group in optimizer.param_groups:\n",
        "          param_group['lr'] = param_group['lr'] / LR_DECAY_VALUE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([50])\n"
          ]
        }
      ],
      "source": [
        "print(y_pred.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTIjyHN-S061"
      },
      "source": [
        "Assess the performance of the model using the test set by predicting the labels and finding a MSE loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPFRkqadVH5v",
        "outputId": "b079f76a-9b10-497e-b595-e3a1f1eaec89"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "test_loss = 0\n",
        "all_y_true = []\n",
        "all_y_pred = []\n",
        "\n",
        "for test_batch in test_loader:\n",
        "    test_batch = test_batch.to(device)\n",
        "    with torch.no_grad():\n",
        "        y_pred = model(test_batch)\n",
        "    y_true = test_batch.y\n",
        "    test_loss += F.mse_loss(y_pred, y_true, reduction='sum').item()\n",
        "\n",
        "    all_y_true.append(y_true.cpu().numpy())\n",
        "    all_y_pred.append(y_pred.cpu().numpy())\n",
        "\n",
        "mse_loss = test_loss / len(test_loader.dataset)\n",
        "\n",
        "all_y_true = np.concatenate(all_y_true)\n",
        "all_y_pred = np.concatenate(all_y_pred)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3 3 3 ... 4 4 4]\n"
          ]
        }
      ],
      "source": [
        "print((all_y_pred).astype('int'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5 3 3 ... 5 3 5]\n"
          ]
        }
      ],
      "source": [
        "print((all_y_true).astype('int'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "zogr_qoXtcGm"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "def personalized_topk(pred, K, user_indices, edge_index):\n",
        "\n",
        "    user_item_pairs = set(zip(edge_index[0], edge_index[1]))\n",
        "\n",
        "    per_user_preds = collections.defaultdict(list)\n",
        "    for index, user in enumerate(user_indices):\n",
        "        per_user_preds[user.item()].append(pred[index].item())\n",
        "\n",
        "    precisions = 0.0\n",
        "    recalls = 0.0\n",
        "    num_users = len(per_user_preds) \n",
        "\n",
        "    for user, preds in per_user_preds.items():\n",
        "        while len(preds) < K:\n",
        "            preds.append(random.choice(range(max(edge_index[1]) + 1)))\n",
        "        top_ratings, top_items = torch.topk(torch.tensor(preds), K)\n",
        "        items_indices = top_items.tolist()\n",
        "\n",
        "        correct_preds = sum((user, item) in user_item_pairs for item in items_indices)\n",
        "        total_pos = sum(1 for item in range(max(edge_index[1]) + 1) if (user, item) in user_item_pairs)\n",
        "\n",
        "        precisions += correct_preds / K\n",
        "        recalls += correct_preds / total_pos if total_pos != 0 else 0\n",
        "\n",
        "    return precisions / num_users, recalls / num_users\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "precision_score, recall_score = personalized_topk(all_y_pred,10,test_u_indices,test_dataset.links)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0673202614379086"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "precision_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "import collections\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def personalized_topk_metrics(pred, K, user_indices, edge_index):\n",
        "\n",
        "    user_item_pairs = set(zip(edge_index[0], edge_index[1]))\n",
        "\n",
        "    per_user_preds = collections.defaultdict(list)\n",
        "    for index, user in enumerate(user_indices):\n",
        "        per_user_preds[user.item()].append((pred[index].item(), index))\n",
        "\n",
        "    ndcgs = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    num_users = len(per_user_preds) \n",
        "\n",
        "    for user, user_preds in per_user_preds.items():\n",
        "\n",
        "        sorted_preds = sorted(user_preds, key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        top_k_preds = sorted_preds[:K]\n",
        "        if len(top_k_preds) < K:\n",
        "            top_k_preds.extend([(0, random.choice(range(max(edge_index[1]) + 1))) for _ in range(K - len(top_k_preds))])\n",
        "\n",
        "        dcg = 0.0\n",
        "        correct_preds = 0\n",
        "        for i, (_, item_index) in enumerate(top_k_preds):\n",
        "            if (user, item_index) in user_item_pairs:\n",
        "                dcg += 1 / torch.log2(torch.tensor(i + 2)).item()  # log base 2 of (i+1+1)\n",
        "                correct_preds += 1\n",
        "\n",
        "        # Calculate IDCG\n",
        "        relevant_items = [item for item in range(max(edge_index[1]) + 1) if (user, item) in user_item_pairs]\n",
        "        idcg = 0.0\n",
        "        for i in range(min(K, len(relevant_items))):\n",
        "            idcg += 1 / torch.log2(torch.tensor(i + 2)).item()\n",
        "\n",
        "        # Calculate NDCG\n",
        "        ndcg = dcg / idcg if idcg > 0 else 0\n",
        "        ndcgs.append(ndcg)\n",
        "\n",
        "        # Calculate precision and recall\n",
        "        total_pos = len(relevant_items)\n",
        "        precision = correct_preds / K if K > 0 else 0\n",
        "        recall = correct_preds / total_pos if total_pos > 0 else 0\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "\n",
        "    # Return the average NDCG, precision, and recall\n",
        "    average_ndcg = sum(ndcgs)\n",
        "    average_precision = sum(precisions) / num_users if precisions else 0\n",
        "    average_recall = sum(recalls) / num_users if recalls else 0\n",
        "\n",
        "    return average_ndcg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 88%|████████▊ | 17604/20000 [1:47:56<14:41,  2.72it/s] \n"
          ]
        }
      ],
      "source": [
        "ndcg_score = personalized_topk_metrics(all_y_pred.astype('int'), 10, test_u_indices.astype('int'), test_dataset.links)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.037239678818512076\n"
          ]
        }
      ],
      "source": [
        "print(ndcg_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
